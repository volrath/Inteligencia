\documentclass[10pt, spanish]{article}
\usepackage[spanish, activeacute]{babel}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}

\begin{document}
\title{Universidad Simón Bolívar \\ Inteligencia II \\ Tarea 1}
\author{
  Daniel Barreto - \#04-36723 \texttt{<daniel.barreto.n@gmail.com>} \\
  Kristoffer Pantic - \#05-38675 \texttt{<kristoffer.pantic@gmail.com>}
}
\maketitle

\section{Resumen}
\label{sec:resumen}
En este proyecto nos propusimos realizar la implementación de un perceptrón
de dos entradas probando el efecto de diferentes algoritmos tanto en el
cálculo de los pesos de cada entrada así como en la variación de la tasa de
aprendizaje (delta rule, descending learning rate y standard gradient descent).
De igual manera para la segunda parte del proyecto nos dispusimos a crear 
una red neural que pudiera clasificar si un punto dado contenido en un rectángulo
en el plano xy se encuentra o no dentro de una circunferencia que se encuentra
dentro del rectángulo. Se realizaron de igual manera pruebas usando un número
variable de neuronas intermedia. A continuación se presentan los detalles de la
implementación y los resultados de las pruebas.

\section{Detalles de Implementación}
\label{sec:di}
Para la realización de los puntos de la tarea se crearon cuatro tipos
abstractos de datos y algunas funciones auxiliares:

\subsection{Tipos abstractos de datos}

\textbf{Perceptron}\\
Esta clase representa una red neural sin capas escondidas y únicamente
con una neurona de salida, que mantiene un vector de pesos para cada
input que recibe. En esta clase se encuentran los métodos necesarios
para evaluar el resultado de un vector de inputs y para entrenarse con
un conjunto de prueba.

La evaluación devuelve un número real que está dado como el producto
punto del vector de inputs y el vector de pesos\\

\textbf{BooleanPerceptron}\\
Esta clase hereda de la clase \emph{Perceptron}, pero sobreescribe la
forma en la que se realiza la evaluación de los inputs para
representar la evaluación de una función \emph{threshold} y devolver
únicamente 0 cuando la evaluación real dé un número no positivo,
o 1 en el caso contrario.\\

\textbf{SigmoidNeuron}\\
Hereda igualmente de la clase \emph{Perceptron} para obtener su
atributo básico de la lista de pesos asociados a una lista de
inputs. La diferencia entre \emph{SigmoidNeuron} y \emph{Perceptron}
se encuentra en la evaluación de un input dado, para el cual
\emph{SigmoidNeuron} retorna la función sigmoidal aplicada sobre el
producto punto del vector de pesos y el vector de inputs.\\

\textbf{NeuralNetwork}\\
Utiliza SigmoidNeuron para generar las instancias de todas las
neuronas que conforman la red. Posee 3 atributos: \emph{número de
  inputs}, \emph{lista de neuronas de la capa intermedia}, \emph{lista
  de neuronas de la capa de salida}.  Con dichos atributos se define
el entrenamiento con \emph{backpropagation} como un método de la clase
dónde se utilizan listas de listas de pesos en vez de una matriz de
pesos para representar las fuerzas de las
conexiones entre cada neurona.\\

\subsection{Funciones Auxiliares}
\textbf{\texttt{training(neural\_network, training\_set,
    learning\_rate=.1,\\ max\_iterations=1000, reduce\_rate=False)}}\\
Es la función principal del programa al momento de entrenar una red
neural, se encarga de realizar un máximo de iteraciones
(\texttt{max\_iterations}) sobre un conjunto de entrenamiento
(\texttt{training\_set}) dado, dónde en cada iteración se le pide a la
red neural (\texttt{neural\_network}) que entrene con
\emph{backpropagation} utilizando una taza de aprendizaje
(\texttt{learning\_rate}) y posiblemente una reducción de la misma en
cada iteración (\texttt{reduce\_rate}).\\

\textbf{\texttt{test(neural\_network, test\_set)}}\\
Prueba el rendimiento de una red neural (\texttt{neural\_network})
sobre un conjunto de pruebas (\texttt{test\_set}). Devuelve un par de
listas de puntos; en la primera lista de puntos se encuentran todos
aquellos puntos que forman parte del \emph{área A}, y en la segunda
los que forman parte del\emph{área B}.\\

A parte de estas funciones, existen otras menos importantes como:\\
\texttt{get\_random\_set(set\_size)},
\texttt{load\_training\_set(file\_name)}, \texttt{plot(error\_log,
  test\_log)} que se utilizan para generar conjuntos random de pruebas,
cargar conjuntos de pruebas desde archivos y graficar la salida del
desempeño de una red neural, respectivamente.

\section{Análisis de Resultados}
\label{sec:analisis}

\subsection{Parte 1: Perceptrones}

\subsubsection{Pregunta 1}
Al evaluar los resultados del perceptrón para las funciones booleanas 
AND, OR y XOR se puede apreciar que tanto AND como OR pueden ser
simuladas con un perceptrón entrenado en cuestión de pocas iteraciones 
pero XOR no puede ya que es una función que no es linealmente separable,
lo que significa que no podemos definir un plano que separe los resultados
positivos de los negativos de la función.
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/p11a-AND.png}
  \caption{Error en función de la iteración en la función AND}\label{fig:nodos}
\end{figure}
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/p11a-OR.png}
  \caption{Error en función de la iteración en la función OR}\label{fig:nodos}
\end{figure}
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/p11a-XOR.png}
  \caption{Error en función de la iteración en la función XOR}\label{fig:nodos}
\end{figure}
\subsubsection{Pregunta 1.a}
Intentando todos los distintos valores para la tasa de aprendizaje $\letter \eta$
se ve que este no hace ninguna diferencia en cuanto al número de iteraciones
necesarias para que el error en el perceptrón converga hacia su mínimo.
\subsubsection{Pregunta 2}
Se obtienen resultados similares a los de la pregunta 1 pero si no se establece
un borde o "threshold" a partir del cual se asigna el valor 0 o 1 a las salidas
no se obtiene un valor identico a la salida esperada.
\subsubsection{Pregunta 2.a}
Se pudo observar que para tasas de aprendizaje pequeñas los resultados 
eran mucho mejores sin la tasa de aprendizaje decayente pero en el caso
de tasas de aprendizaje mayores se aprecia como el cambio de la tasa
permite que a medida que va avanzando el algoritmo cada ejemplo altere
menos los pesos del perceptrón y en consecuencia que converga más rapido
e inclusive en algunos casos logra conseguir resultados imposible sin
una tasa de aprendizaje decreciente como es el caso de $\letter \eta = 0.5$.
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/p12a-AND.png}
  \caption{Comparación del error al usar una tasa de aprendizaje
    decreciente con varias tasas iniciales: 0.1, 0.5 y
    0.99}\label{fig:nodos}
\end{figure}
\subsubsection{Pregunta 2.b}
Se puede ver en las gráficas que el aprendizaje por lotes converge más rapido,
requiere más tiempo de ejecución pero al mismo tiempo requiere menos 
actualizaciones de pesos que el aprendizaje incremental ya que en el incremental
se actualizan los pesos por cada ejemplo del "training set" y por lotes se 
actualizan por cada iteración pero requieren más cálculos. Es importante
resaltar que en el aprendizaje por lotes se vuelve crucial la selección de la
tasa de aprendizaje ya que una tasa de aprendizaje muy grande puede
llevar al algoritmo a saltarse el mínimo global de la función y a diverger 
hacia un error inmenso.
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/p12b-AND-SGD.png}
  \caption{Gráfica del error de AND al usar aprendizaje por lotes}\label{fig:nodos}
\end{figure}

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/p12b-AND-NoSGD.png}
  \caption{Gráfica del error de AND al usar aprendizaje incremental}\label{fig:nodos}
\end{figure}


\subsection{Parte 2: Red neural y backpropagation}
Se realizaron entrenamientos con redes de 2 a 10 neuronas en la capa
intermedia sobre los 6 conjuntos especificados en el enunciado. Los
conjuntos utilizados son mostrados en las figuras \ref{fig:ts1},
\ref{fig:ts2}, \ref{fig:ts3}, \ref{fig:ts4}, \ref{fig:ts5},
\ref{fig:ts6}.

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/500.png}
  \caption{500 puntos uniformemente repartidos} \label{fig:ts1}
\end{figure}

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/1000.png}
  \caption{1000 puntos uniformemente repartidos}\label{fig:ts2}
\end{figure}

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/2000.png}
  \caption{2000 puntos uniformemente repartidos}\label{fig:ts3}
\end{figure}

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/500e.png}
  \caption{500 puntos equivalentemente repartidos}\label{fig:ts4}
\end{figure}

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/1000e.png}
  \caption{1000 puntos equivalentemente repartidos}\label{fig:ts5}
\end{figure}

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/2000e.png}
  \caption{2000 puntos equivalentemente repartidos}\label{fig:ts6}
\end{figure}

Los mejores resultados obtenidos en cada conjunto, probando contra
conjuntos de prueba de 10000 puntos generados aleatoriamente sobre
toda la superficie, se muestran a continuación.

\newpage
\subsection{500 puntos uniformemente repartidos}
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/error-derivation-500.png}
  \caption{Errores con 500 puntos uniformemente repartidos}\label{fig:ed1}
\end{figure}

Mejor resultado obtenido con \textbf{8 neuronas} intermedias.\\
Porcentaje de error: \textbf{5.83\%}

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/500-8-best-performance.png}
  \caption{Mejor resultado con 8 neuronas en la capa intermedia}\label{fig:nodos}
\end{figure}

\newpage
\subsection{1000 puntos uniformemente repartidos}
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/error-derivation-1000.png}
  \caption{Errores con 1000 puntos uniformemente repartidos}\label{fig:nodos}
\end{figure}

Mejor resultado obtenido con \textbf{10 neuronas} intermedias.\\
Porcentaje de error: \textbf{2.86\%}

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/1000-10-best-performance.png}
  \caption{Mejor resultado con 10 neuronas en la capa intermedia}\label{fig:nodos}
\end{figure}

\newpage
\subsection{2000 puntos uniformemente repartidos}
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/error-derivation-2000.png}
  \caption{Errores con 2000 puntos uniformemente repartidos}\label{fig:nodos}
\end{figure}

Mejor resultado obtenido con \textbf{9 neuronas} intermedias.\\
Porcentaje de error: \textbf{2.41\%}

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/2000-9-best-performance.png}
  \caption{Mejor resultado con 9 neuronas en la capa intermedia}\label{fig:nodos}
\end{figure}

\newpage
\subsection{500 puntos equivalentemente repartidos}
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/error-derivation-500e.png}
  \caption{Errores con 500 puntos equivalentemente repartidos}\label{fig:nodos}
\end{figure}

Mejor resultado obtenido con \textbf{7 neuronas} intermedias.\\
Porcentaje de error: \textbf{17.27\%}

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/500e-7-best-performance.png}
  \caption{Mejor resultado con 7 neuronas en la capa intermedia}\label{fig:nodos}
\end{figure}

\newpage
\subsection{1000 puntos equivalentemente repartidos}
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/error-derivation-1000e.png}
  \caption{Errores con 1000 puntos equivalentemente repartidos}\label{fig:nodos}
\end{figure}

Mejor resultado obtenido con \textbf{8 neuronas} intermedias.\\
Porcentaje de error: \textbf{19.88\%}

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/1000e-8-best-performance.png}
  \caption{Mejor resultado con 8 neuronas en la capa intermedia}\label{fig:nodos}
\end{figure}

\newpage
\subsection{2000 puntos equivalentemente repartidos}
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/error-derivation-2000e.png}
  \caption{Errores con 2000 puntos equivalentemente repartidos}\label{fig:nodos}
\end{figure}

Mejor resultado obtenido con \textbf{10 neuronas} intermedias.\\
Porcentaje de error: \textbf{20.26\%}

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{media/2000e-10-best-performance.png}
  \caption{Mejor resultado con 10 neuronas en la capa intermedia}\label{fig:nodos}
\end{figure}

\end{document}
